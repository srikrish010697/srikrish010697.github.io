---
layout: post
title:  Interpretable counterfactual explanations for object detection models from joint multimodal (visual,language) representations
date: 2023-06-14 12:00:00-0400
description: Currently working on an unbiased, robust, interpretability technique using counter factual explanations from multimodal embedded representations. The goal of concept-based explanation is to provide concise and understandable explanations for an object classifier. However, current methods for concept-based explanations often require a large number of manually annotated images, which is expensive and introduces the risk of human biases in the explanations. In this research, we introduce a language-driven counterfactual explanation generator. The proposed method defines concepts solely from text using a pre-trained multi-modal joint embedding space, eliminating the need for additional concept-annotated datasets. To interpret the outcomes of the target classifier using these text-driven concepts, we aim to propose a novel projection scheme that maps the two spaces together using a simple yet effective implementation. 
categories: Explainable-AI, Trustworthy-AI, Causal-inference, counterfactual-explanations, language-grounding
disqus_comments: false
related_posts: false
---

