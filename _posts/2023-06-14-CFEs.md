---
layout: post
title:  Interpretable counterfactual explanations for object detection models using generative AI
date: 2023-08-16 5:30:00-0400
description: In recent times, deep vision models have gained extensive usage in safety-critical applications like autonomous driving, raising concerns about the interpretability of these models. Among various explanation techniques, counterfactual explanations are designed to identify small and understandable alterations to an input image that would lead to a different output from the model being explained. Such explanations highlight the key factors influencing the model's decision, aiding end-users in comprehending the decision-making process. However, existing methods face challenges when explaining decision models trained on complex images with multiple objects, such as urban scenes. These challenging images are both more intricate to work with and more crucial to explain. In this research, a solution is proposed to address this issue using an object-centric framework for generating counterfactual explanations. The method draws inspiration from recent generative modeling approaches and encodes the original image into a structured latent space that facilitates manipulation at the object level. 
categories: Explainable-AI, counterfactual-explanations, generative AI, computer vision
disqus_comments: false
related_posts: false
---

